{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“欢迎使用 Colaboratory”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TS132000/-/blob/master/%E2%80%9C%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8_Colaboratory%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD24TMPa07Iu",
        "colab_type": "text"
      },
      "source": [
        "#**Seq2Seq and Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjJnnFWiIeOJ",
        "colab_type": "text"
      },
      "source": [
        "#**导入相关包**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj0L4hy31NM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "from typing import NamedTuple\n",
        "import torch\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZrOnXfDIidM",
        "colab_type": "text"
      },
      "source": [
        "##**boolmask模块**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHClzWqZIwIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _pad_mask(mask):\n",
        "    # By taking -size % 8, we get 0 if exactly divisible by 8\n",
        "    # and required padding otherwise (i.e. -1 % 8 = 7 pad)\n",
        "    pad = -mask.size(-1) % 8\n",
        "    if pad != 0:\n",
        "        mask = F.pad(mask, [0, pad])\n",
        "    return mask, mask.size(-1) // 8\n",
        "\n",
        "\n",
        "def _mask_bool2byte(mask):\n",
        "    assert mask.dtype == torch.uint8\n",
        "    # assert (mask <= 1).all()  # Precondition, disabled for efficiency\n",
        "    mask, d = _pad_mask(mask)\n",
        "    return (mask.view(*mask.size()[:-1], d, 8) << torch.arange(8, out=mask.new())).sum(-1, dtype=torch.uint8)\n",
        "\n",
        "\n",
        "def _mask_byte2long(mask):\n",
        "    assert mask.dtype == torch.uint8\n",
        "    mask, d = _pad_mask(mask)\n",
        "    # Note this corresponds to a temporary factor 8\n",
        "    # memory overhead by converting to long before summing\n",
        "    # Alternatively, aggregate using for loop\n",
        "    return (mask.view(*mask.size()[:-1], d, 8).long() << (torch.arange(8, dtype=torch.int64, device=mask.device) * 8)).sum(-1)\n",
        "\n",
        "\n",
        "def mask_bool2long(mask):\n",
        "    assert mask.dtype == torch.uint8\n",
        "    return _mask_byte2long(_mask_bool2byte(mask))\n",
        "\n",
        "\n",
        "def _mask_long2byte(mask, n=None):\n",
        "    if n is None:\n",
        "        n = 8 * mask.size(-1)\n",
        "    return (mask[..., None] >> (torch.arange(8, out=mask.new()) * 8))[..., :n].to(torch.uint8).view(*mask.size()[:-1], -1)[..., :n]\n",
        "\n",
        "\n",
        "def _mask_byte2bool(mask, n=None):\n",
        "    if n is None:\n",
        "        n = 8 * mask.size(-1)\n",
        "    return (mask[..., None] & (mask.new_ones(8) << torch.arange(8, out=mask.new()) * 1)).view(*mask.size()[:-1], -1)[..., :n] > 0\n",
        "\n",
        "\n",
        "def mask_long2bool(mask, n=None):\n",
        "    assert mask.dtype == torch.int64\n",
        "    return _mask_byte2bool(_mask_long2byte(mask), n=n)\n",
        "\n",
        "\n",
        "def mask_long_scatter(mask, values, check_unset=True):\n",
        "    \"\"\"\n",
        "    Sets values in mask in dimension -1 with arbitrary batch dimensions\n",
        "    If values contains -1, nothing is set\n",
        "    Note: does not work for setting multiple values at once (like normal scatter)\n",
        "    \"\"\"\n",
        "    assert mask.size()[:-1] == values.size()\n",
        "    rng = torch.arange(mask.size(-1), out=mask.new())\n",
        "    values_ = values[..., None]  # Need to broadcast up do mask dim\n",
        "    # This indicates in which value of the mask a bit should be set\n",
        "    where = (values_ >= (rng * 64)) & (values_ < ((rng + 1) * 64))\n",
        "    # Optional: check that bit is not already set\n",
        "    assert not (check_unset and ((mask & (where.long() << (values_ % 64))) > 0).any())\n",
        "    # Set bit by shifting a 1 to the correct position\n",
        "    # (% not strictly necessary as bitshift is cyclic)\n",
        "    # since where is 0 if no value needs to be set, the bitshift has no effect\n",
        "    return mask | (where.long() << (values_ % 64))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztT_IFyoI2Oh",
        "colab_type": "text"
      },
      "source": [
        "#**beamsearch模块**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_S9Lky4JVQZ",
        "colab_type": "code",
        "outputId": "f7a609ad-2a93-47ca-c046-490aad5e152b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def _pad_mask(mask):\n",
        "    # By taking -size % 8, we get 0 if exactly divisible by 8\n",
        "    # and required padding otherwise (i.e. -1 % 8 = 7 pad)\n",
        "    pad = -mask.size(-1) % 8\n",
        "    if pad != 0:\n",
        "        mask = F.pad(mask, [0, pad])\n",
        "    return mask, mask.size(-1) // 8\n",
        "\n",
        "\n",
        "def _mask_bool2byte(mask):\n",
        "    assert mask.dtype == torch.uint8\n",
        "    # assert (mask <= 1).all()  # Precondition, disabled for efficiency\n",
        "    mask, d = _pad_mask(mask)\n",
        "    return (mask.view(*mask.size()[:-1], d, 8) << torch.arange(8, out=mask.new())).sum(-1, dtype=torch.uint8)\n",
        "\n",
        "\n",
        "def _mask_byte2long(mask):\n",
        "    assert mask.dtype == torch.uint8\n",
        "    mask, d = _pad_mask(mask)\n",
        "    # Note this corresponds to a temporary factor 8\n",
        "    # memory overhead by converting to long before summing\n",
        "    # Alternatively, aggregate using for loop\n",
        "    return (mask.view(*mask.size()[:-1], d, 8).long() << (torch.arange(8, dtype=torch.int64, device=mask.device) * 8)).sum(-1)\n",
        "\n",
        "\n",
        "def mask_bool2long(mask):\n",
        "    assert mask.dtype == torch.uint8\n",
        "    return _mask_byte2long(_mask_bool2byte(mask))\n",
        "\n",
        "\n",
        "def _mask_long2byte(mask, n=None):\n",
        "    if n is None:\n",
        "        n = 8 * mask.size(-1)\n",
        "    return (mask[..., None] >> (torch.arange(8, out=mask.new()) * 8))[..., :n].to(torch.uint8).view(*mask.size()[:-1], -1)[..., :n]\n",
        "\n",
        "\n",
        "def _mask_byte2bool(mask, n=None):\n",
        "    if n is None:\n",
        "        n = 8 * mask.size(-1)\n",
        "    return (mask[..., None] & (mask.new_ones(8) << torch.arange(8, out=mask.new()) * 1)).view(*mask.size()[:-1], -1)[..., :n] > 0\n",
        "\n",
        "\n",
        "def mask_long2bool(mask, n=None):\n",
        "    assert mask.dtype == torch.int64\n",
        "    return _mask_byte2bool(_mask_long2byte(mask), n=n)\n",
        "\n",
        "\n",
        "def mask_long_scatter(mask, values, check_unset=True):\n",
        "    \"\"\"\n",
        "    Sets values in mask in dimension -1 with arbitrary batch dimensions\n",
        "    If values contains -1, nothing is set\n",
        "    Note: does not work for setting multiple values at once (like normal scatter)\n",
        "    \"\"\"\n",
        "    assert mask.size()[:-1] == values.size()\n",
        "    rng = torch.arange(mask.size(-1), out=mask.new())\n",
        "    values_ = values[..., None]  # Need to broadcast up do mask dim\n",
        "    # This indicates in which value of the mask a bit should be set\n",
        "    where = (values_ >= (rng * 64)) & (values_ < ((rng + 1) * 64))\n",
        "    # Optional: check that bit is not already set\n",
        "    assert not (check_unset and ((mask & (where.long() << (values_ % 64))) > 0).any())\n",
        "    # Set bit by shifting a 1 to the correct position\n",
        "    # (% not strictly necessary as bitshift is cyclic)\n",
        "    # since where is 0 if no value needs to be set, the bitshift has no effect\n",
        "    return mask | (where.long() << (values_ % 64))\n",
        "import time\n",
        "import torch\n",
        "from typing import NamedTuple\n",
        "\n",
        "\n",
        "def beam_search(*args, **kwargs):\n",
        "    beams, final_state = _beam_search(*args, **kwargs)\n",
        "    return get_beam_search_results(beams, final_state)\n",
        "\n",
        "\n",
        "def get_beam_search_results(beams, final_state):\n",
        "    beam = beams[-1]  # Final beam\n",
        "    if final_state is None:\n",
        "        return None, None, None, None, beam.batch_size\n",
        "\n",
        "    # First state has no actions/parents and should be omitted when backtracking\n",
        "    actions = [beam.action for beam in beams[1:]]\n",
        "    parents = [beam.parent for beam in beams[1:]]\n",
        "\n",
        "    solutions = final_state.construct_solutions(backtrack(parents, actions))\n",
        "    return beam.score, solutions, final_state.get_final_cost()[:, 0], final_state.ids.view(-1), beam.batch_size\n",
        "\n",
        "\n",
        "def _beam_search(state, beam_size, propose_expansions=None,\n",
        "                keep_states=False):\n",
        "\n",
        "    beam = BatchBeam.initialize(state)\n",
        "\n",
        "    # Initial state\n",
        "    beams = [beam if keep_states else beam.clear_state()]\n",
        "\n",
        "    # Perform decoding steps\n",
        "    while not beam.all_finished():\n",
        "\n",
        "        # Use the model to propose and score expansions\n",
        "        parent, action, score = beam.propose_expansions() if propose_expansions is None else propose_expansions(beam)\n",
        "        if parent is None:\n",
        "            return beams, None\n",
        "\n",
        "        # Expand and update the state according to the selected actions\n",
        "        beam = beam.expand(parent, action, score=score)\n",
        "\n",
        "        # Get topk\n",
        "        beam = beam.topk(beam_size)\n",
        "\n",
        "        # Collect output of step\n",
        "        beams.append(beam if keep_states else beam.clear_state())\n",
        "\n",
        "    # Return the final state separately since beams may not keep state\n",
        "    return beams, beam.state\n",
        "\n",
        "\n",
        "class BatchBeam(NamedTuple):\n",
        "    \"\"\"\n",
        "    Class that keeps track of a beam for beam search in batch mode.\n",
        "    Since the beam size of different entries in the batch may vary, the tensors are not (batch_size, beam_size, ...)\n",
        "    but rather (sum_i beam_size_i, ...), i.e. flattened. This makes some operations a bit cumbersome.\n",
        "    \"\"\"\n",
        "    score: torch.Tensor  # Current heuristic score of each entry in beam (used to select most promising)\n",
        "    state: None  # To track the state\n",
        "    parent: torch.Tensor\n",
        "    action: torch.Tensor\n",
        "    batch_size: int  # Can be used for optimizations if batch_size = 1\n",
        "    device: None  # Track on which device\n",
        "\n",
        "    # Indicates for each row to which batch it belongs (0, 0, 0, 1, 1, 2, ...), managed by state\n",
        "    @property\n",
        "    def ids(self):\n",
        "        return self.state.ids.view(-1)  # Need to flat as state has steps dimension\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if torch.is_tensor(key) or isinstance(key, slice):  # If tensor, idx all tensors by this tensor:\n",
        "            return self._replace(\n",
        "                # ids=self.ids[key],\n",
        "                score=self.score[key] if self.score is not None else None,\n",
        "                state=self.state[key],\n",
        "                parent=self.parent[key] if self.parent is not None else None,\n",
        "                action=self.action[key] if self.action is not None else None\n",
        "            )\n",
        "        return super(BatchBeam, self).__getitem__(key)\n",
        "\n",
        "    # Do not use __len__ since this is used by namedtuple internally and should be number of fields\n",
        "    # def __len__(self):\n",
        "    #     return len(self.ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def initialize(state):\n",
        "        batch_size = len(state.ids)\n",
        "        device = state.ids.device\n",
        "        return BatchBeam(\n",
        "            score=torch.zeros(batch_size, dtype=torch.float, device=device),\n",
        "            state=state,\n",
        "            parent=None,\n",
        "            action=None,\n",
        "            batch_size=batch_size,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    def propose_expansions(self):\n",
        "        mask = self.state.get_mask()\n",
        "        # Mask always contains a feasible action\n",
        "        expansions = torch.nonzero(mask[:, 0, :] == 0)\n",
        "        parent, action = torch.unbind(expansions, -1)\n",
        "        return parent, action, None\n",
        "\n",
        "    def expand(self, parent, action, score=None):\n",
        "        return self._replace(\n",
        "            score=score,  # The score is cleared upon expanding as it is no longer valid, or it must be provided\n",
        "            state=self.state[parent].update(action),  # Pass ids since we replicated state\n",
        "            parent=parent,\n",
        "            action=action\n",
        "        )\n",
        "\n",
        "    def topk(self, k):\n",
        "        idx_topk = segment_topk_idx(self.score, k, self.ids)\n",
        "        return self[idx_topk]\n",
        "\n",
        "    def all_finished(self):\n",
        "        return self.state.all_finished()\n",
        "\n",
        "    def cpu(self):\n",
        "        return self.to(torch.device('cpu'))\n",
        "\n",
        "    def to(self, device):\n",
        "        if device == self.device:\n",
        "            return self\n",
        "        return self._replace(\n",
        "            score=self.score.to(device) if self.score is not None else None,\n",
        "            state=self.state.to(device),\n",
        "            parent=self.parent.to(device) if self.parent is not None else None,\n",
        "            action=self.action.to(device) if self.action is not None else None\n",
        "        )\n",
        "\n",
        "    def clear_state(self):\n",
        "        return self._replace(state=None)\n",
        "\n",
        "    def size(self):\n",
        "        return self.state.ids.size(0)\n",
        "\n",
        "\n",
        "def segment_topk_idx(x, k, ids):\n",
        "    \"\"\"\n",
        "    Finds the topk per segment of data x given segment ids (0, 0, 0, 1, 1, 2, ...).\n",
        "    Note that there may be fewer than k elements in a segment so the returned length index can vary.\n",
        "    x[result], ids[result] gives the sorted elements per segment as well as corresponding segment ids after sorting.\n",
        "    :param x:\n",
        "    :param k:\n",
        "    :param ids:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    assert x.dim() == 1\n",
        "    assert ids.dim() == 1\n",
        "\n",
        "    # Since we may have varying beam size per batch entry we cannot reshape to (batch_size, beam_size)\n",
        "    # And use default topk along dim -1, so we have to be creative\n",
        "    # Now we have to get the topk per segment which is really annoying :(\n",
        "    # we use lexsort on (ids, score), create array with offset per id\n",
        "    # offsets[ids] then gives offsets repeated and only keep for which arange(len) < offsets + k\n",
        "    splits_ = torch.nonzero(ids[1:] - ids[:-1])\n",
        "\n",
        "    if len(splits_) == 0:  # Only one group\n",
        "        _, idx_topk = x.topk(min(k, x.size(0)))\n",
        "        return idx_topk\n",
        "\n",
        "    splits = torch.cat((ids.new_tensor([0]), splits_[:, 0] + 1))\n",
        "    # Make a new array in which we store for each id the offset (start) of the group\n",
        "    # This way ids does not need to be increasing or adjacent, as long as each group is a single range\n",
        "    group_offsets = splits.new_zeros((splits.max() + 1,))\n",
        "    group_offsets[ids[splits]] = splits\n",
        "    offsets = group_offsets[ids]  # Look up offsets based on ids, effectively repeating for the repetitions per id\n",
        "\n",
        "    # We want topk so need to sort x descending so sort -x (be careful with unsigned data type!)\n",
        "    idx_sorted = torch_lexsort((-(x if x.dtype != torch.uint8 else x.int()).detach(), ids))\n",
        "\n",
        "    # This will filter first k per group (example k = 2)\n",
        "    # ids     = [0, 0, 0, 1, 1, 1, 1, 2]\n",
        "    # splits  = [0, 3, 7]\n",
        "    # offsets = [0, 0, 0, 3, 3, 3, 3, 7]\n",
        "    # offs+2  = [2, 2, 2, 5, 5, 5, 5, 9]\n",
        "    # arange  = [0, 1, 2, 3, 4, 5, 6, 7]\n",
        "    # filter  = [1, 1, 0, 1, 1, 0, 0, 1]\n",
        "    # Use filter to get only topk of sorting idx\n",
        "    return idx_sorted[torch.arange(ids.size(0), out=ids.new()) < offsets + k]\n",
        "\n",
        "\n",
        "def backtrack(parents, actions):\n",
        "\n",
        "    # Now backtrack to find aligned action sequences in reversed order\n",
        "    cur_parent = parents[-1]\n",
        "    reversed_aligned_sequences = [actions[-1]]\n",
        "    for parent, sequence in reversed(list(zip(parents[:-1], actions[:-1]))):\n",
        "        reversed_aligned_sequences.append(sequence.gather(-1, cur_parent))\n",
        "        cur_parent = parent.gather(-1, cur_parent)\n",
        "\n",
        "    return torch.stack(list(reversed(reversed_aligned_sequences)), -1)\n",
        "\n",
        "\n",
        "class CachedLookup(object):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.orig = data\n",
        "        self.key = None\n",
        "        self.current = None\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        assert not isinstance(key, slice), \"CachedLookup does not support slicing, \" \\\n",
        "                                           \"you can slice the result of an index operation instead\"\n",
        "\n",
        "        if torch.is_tensor(key):  # If tensor, idx all tensors by this tensor:\n",
        "\n",
        "            if self.key is None:\n",
        "                self.key = key\n",
        "                self.current = self.orig[key]\n",
        "            elif len(key) != len(self.key) or (key != self.key).any():\n",
        "                self.key = key\n",
        "                self.current = self.orig[key]\n",
        "\n",
        "            return self.current\n",
        "\n",
        "        return super(CachedLookup, self).__getitem__(key)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:121: DeprecationWarning: __class__ not set defining 'BatchBeam' as <class '__main__.BatchBeam'>. Was __classcell__ propagated to type.__new__?\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5vRIdOa_2cW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "import pickle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYOg1_5LKR5N",
        "colab_type": "text"
      },
      "source": [
        "##**StateCVRP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhQrTiQv_2os",
        "colab_type": "code",
        "outputId": "438d040f-065a-4ee9-a9fa-188b67bd69b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "\n",
        "class StateCVRP(NamedTuple):\n",
        "    # Fixed input\n",
        "    coords: torch.Tensor  # Depot + loc\n",
        "    demand: torch.Tensor\n",
        "\n",
        "    # If this state contains multiple copies (i.e. beam search) for the same instance, then for memory efficiency\n",
        "    # the coords and demands tensors are not kept multiple times, so we need to use the ids to index the correct rows.\n",
        "    ids: torch.Tensor  # Keeps track of original fixed data index of rows\n",
        "\n",
        "    # State\n",
        "    prev_a: torch.Tensor\n",
        "    used_capacity: torch.Tensor\n",
        "    visited_: torch.Tensor  # Keeps track of nodes that have been visited\n",
        "    lengths: torch.Tensor\n",
        "    cur_coord: torch.Tensor\n",
        "    i: torch.Tensor  # Keeps track of step\n",
        "\n",
        "    VEHICLE_CAPACITY = 1.0  # Hardcoded\n",
        "\n",
        "    @property\n",
        "    def visited(self):\n",
        "        if self.visited_.dtype == torch.uint8:\n",
        "            return self.visited_\n",
        "        else:\n",
        "            return mask_long2bool(self.visited_, n=self.demand.size(-1))\n",
        "\n",
        "    @property\n",
        "    def dist(self):\n",
        "        return (self.coords[:, :, None, :] - self.coords[:, None, :, :]).norm(p=2, dim=-1)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if torch.is_tensor(key) or isinstance(key, slice):  # If tensor, idx all tensors by this tensor:\n",
        "            return self._replace(\n",
        "                ids=self.ids[key],\n",
        "                prev_a=self.prev_a[key],\n",
        "                used_capacity=self.used_capacity[key],\n",
        "                visited_=self.visited_[key],\n",
        "                lengths=self.lengths[key],\n",
        "                cur_coord=self.cur_coord[key],\n",
        "            )\n",
        "        return super(StateCVRP, self).__getitem__(key)\n",
        "\n",
        "    # Warning: cannot override len of NamedTuple, len should be number of fields, not batch size\n",
        "    # def __len__(self):\n",
        "    #     return len(self.used_capacity)\n",
        "\n",
        "    @staticmethod\n",
        "    def initialize(input, visited_dtype=torch.uint8):\n",
        "\n",
        "        depot = input['depot']\n",
        "        loc = input['loc']\n",
        "        demand = input['demand']\n",
        "\n",
        "        batch_size, n_loc, _ = loc.size()\n",
        "        return StateCVRP(\n",
        "            coords=torch.cat((depot[:, None, :], loc), -2),\n",
        "            demand=demand,\n",
        "            ids=torch.arange(batch_size, dtype=torch.int64, device=loc.device)[:, None],  # Add steps dimension\n",
        "            prev_a=torch.zeros(batch_size, 1, dtype=torch.long, device=loc.device),\n",
        "            used_capacity=demand.new_zeros(batch_size, 1),\n",
        "            visited_=(  # Visited as mask is easier to understand, as long more memory efficient\n",
        "                # Keep visited_ with depot so we can scatter efficiently\n",
        "                torch.zeros(\n",
        "                    batch_size, 1, n_loc + 1,\n",
        "                    dtype=torch.uint8, device=loc.device\n",
        "                )\n",
        "                if visited_dtype == torch.uint8\n",
        "                else torch.zeros(batch_size, 1, (n_loc + 63) // 64, dtype=torch.int64, device=loc.device)  # Ceil\n",
        "            ),\n",
        "            lengths=torch.zeros(batch_size, 1, device=loc.device),\n",
        "            cur_coord=input['depot'][:, None, :],  # Add step dimension\n",
        "            i=torch.zeros(1, dtype=torch.int64, device=loc.device)  # Vector with length num_steps\n",
        "        )\n",
        "\n",
        "    def get_final_cost(self):\n",
        "\n",
        "        assert self.all_finished()\n",
        "\n",
        "        return self.lengths + (self.coords[self.ids, 0, :] - self.cur_coord).norm(p=2, dim=-1)\n",
        "\n",
        "    def update(self, selected):\n",
        "\n",
        "        assert self.i.size(0) == 1, \"Can only update if state represents single step\"\n",
        "\n",
        "        # Update the state\n",
        "        selected = selected[:, None]  # Add dimension for step\n",
        "        prev_a = selected\n",
        "        n_loc = self.demand.size(-1)  # Excludes depot\n",
        "\n",
        "        # Add the length\n",
        "        cur_coord = self.coords[self.ids, selected]\n",
        "        # cur_coord = self.coords.gather(\n",
        "        #     1,\n",
        "        #     selected[:, None].expand(selected.size(0), 1, self.coords.size(-1))\n",
        "        # )[:, 0, :]\n",
        "        lengths = self.lengths + (cur_coord - self.cur_coord).norm(p=2, dim=-1)  # (batch_dim, 1)\n",
        "\n",
        "        # Not selected_demand is demand of first node (by clamp) so incorrect for nodes that visit depot!\n",
        "        #selected_demand = self.demand.gather(-1, torch.clamp(prev_a - 1, 0, n_loc - 1))\n",
        "        selected_demand = self.demand[self.ids, torch.clamp(prev_a - 1, 0, n_loc - 1)]\n",
        "\n",
        "        # Increase capacity if depot is not visited, otherwise set to 0\n",
        "        #used_capacity = torch.where(selected == 0, 0, self.used_capacity + selected_demand)\n",
        "        used_capacity = (self.used_capacity + selected_demand) * (prev_a != 0).float()\n",
        "\n",
        "        if self.visited_.dtype == torch.uint8:\n",
        "            # Note: here we do not subtract one as we have to scatter so the first column allows scattering depot\n",
        "            # Add one dimension since we write a single value\n",
        "            visited_ = self.visited_.scatter(-1, prev_a[:, :, None], 1)\n",
        "        else:\n",
        "            # This works, will not set anything if prev_a -1 == -1 (depot)\n",
        "            visited_ = mask_long_scatter(self.visited_, prev_a - 1)\n",
        "\n",
        "        return self._replace(\n",
        "            prev_a=prev_a, used_capacity=used_capacity, visited_=visited_,\n",
        "            lengths=lengths, cur_coord=cur_coord, i=self.i + 1\n",
        "        )\n",
        "\n",
        "    def all_finished(self):\n",
        "        return self.i.item() >= self.demand.size(-1) and self.visited.all()\n",
        "\n",
        "    def get_finished(self):\n",
        "        return self.visited.sum(-1) == self.visited.size(-1)\n",
        "\n",
        "    def get_current_node(self):\n",
        "        return self.prev_a\n",
        "\n",
        "    def get_mask(self):\n",
        "        \"\"\"\n",
        "        Gets a (batch_size, n_loc + 1) mask with the feasible actions (0 = depot), depends on already visited and\n",
        "        remaining capacity. 0 = feasible, 1 = infeasible\n",
        "        Forbids to visit depot twice in a row, unless all nodes have been visited\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        if self.visited_.dtype == torch.uint8:\n",
        "            visited_loc = self.visited_[:, :, 1:]\n",
        "        else:\n",
        "            visited_loc = mask_long2bool(self.visited_, n=self.demand.size(-1))\n",
        "\n",
        "        # For demand steps_dim is inserted by indexing with id, for used_capacity insert node dim for broadcasting\n",
        "        exceeds_cap = (self.demand[self.ids, :] + self.used_capacity[:, :, None] > self.VEHICLE_CAPACITY)\n",
        "        # Nodes that cannot be visited are already visited or too much demand to be served now\n",
        "        mask_loc = visited_loc.to(exceeds_cap.dtype) | exceeds_cap\n",
        "\n",
        "        # Cannot visit the depot if just visited and still unserved nodes\n",
        "        mask_depot = (self.prev_a == 0) & ((mask_loc == 0).int().sum(-1) > 0)\n",
        "        return torch.cat((mask_depot[:, :, None], mask_loc), -1)\n",
        "\n",
        "    def construct_solutions(self, actions):\n",
        "        return actions\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: __class__ not set defining 'StateCVRP' as <class '__main__.StateCVRP'>. Was __classcell__ propagated to type.__new__?\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKgpeNcpKl9P",
        "colab_type": "text"
      },
      "source": [
        "# **vrp问题**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCjLsA7WKlQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CVRP(object):\n",
        "\n",
        "    NAME = 'cvrp'  # Capacitated Vehicle Routing Problem\n",
        "\n",
        "    VEHICLE_CAPACITY = 1.0  # (w.l.o.g. vehicle capacity is 1, demands should be scaled)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_costs(dataset, pi):\n",
        "        batch_size, graph_size = dataset['demand'].size()\n",
        "        # Check that tours are valid, i.e. contain 0 to n -1\n",
        "        sorted_pi = pi.data.sort(1)[0]\n",
        "\n",
        "        # Sorting it should give all zeros at front and then 1...n\n",
        "        assert (\n",
        "            torch.arange(1, graph_size + 1, out=pi.data.new()).view(1, -1).expand(batch_size, graph_size) ==\n",
        "            sorted_pi[:, -graph_size:]\n",
        "        ).all() and (sorted_pi[:, :-graph_size] == 0).all(), \"Invalid tour\"\n",
        "\n",
        "        # Visiting depot resets capacity so we add demand = -capacity (we make sure it does not become negative)\n",
        "        demand_with_depot = torch.cat(\n",
        "            (\n",
        "                torch.full_like(dataset['demand'][:, :1], -CVRP.VEHICLE_CAPACITY),\n",
        "                dataset['demand']\n",
        "            ),\n",
        "            1\n",
        "        )\n",
        "        d = demand_with_depot.gather(1, pi)\n",
        "\n",
        "        used_cap = torch.zeros_like(dataset['demand'][:, 0])\n",
        "        for i in range(pi.size(1)):\n",
        "            used_cap += d[:, i]  # This will reset/make capacity negative if i == 0, e.g. depot visited\n",
        "            # Cannot use less than 0\n",
        "            used_cap[used_cap < 0] = 0\n",
        "            assert (used_cap <= CVRP.VEHICLE_CAPACITY + 1e-5).all(), \"Used more than capacity\"\n",
        "\n",
        "        # Gather dataset in order of tour\n",
        "        loc_with_depot = torch.cat((dataset['depot'][:, None, :], dataset['loc']), 1)\n",
        "        d = loc_with_depot.gather(1, pi[..., None].expand(*pi.size(), loc_with_depot.size(-1)))\n",
        "\n",
        "        # Length is distance (L2-norm of difference) of each next location to its prev and of first and last to depot\n",
        "        return (\n",
        "            (d[:, 1:] - d[:, :-1]).norm(p=2, dim=2).sum(1)\n",
        "            + (d[:, 0] - dataset['depot']).norm(p=2, dim=1)  # Depot to first\n",
        "            + (d[:, -1] - dataset['depot']).norm(p=2, dim=1)  # Last to depot, will be 0 if depot is last\n",
        "        ), None\n",
        "\n",
        "    @staticmethod\n",
        "    def make_dataset(*args, **kwargs):\n",
        "        return VRPDataset(*args, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_state(*args, **kwargs):\n",
        "        return StateCVRP.initialize(*args, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def beam_search(input, beam_size, expand_size=None,\n",
        "                    compress_mask=False, model=None, max_calc_batch_size=4096):\n",
        "\n",
        "        assert model is not None, \"Provide model\"\n",
        "\n",
        "        fixed = model.precompute_fixed(input)\n",
        "\n",
        "        def propose_expansions(beam):\n",
        "            return model.propose_expansions(\n",
        "                beam, fixed, expand_size, normalize=True, max_calc_batch_size=max_calc_batch_size\n",
        "            )\n",
        "\n",
        "        state = CVRP.make_state(\n",
        "            input, visited_dtype=torch.int64 if compress_mask else torch.uint8\n",
        "        )\n",
        "\n",
        "        return beam_search(state, beam_size, propose_expansions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9eQ5XSFMpyu",
        "colab_type": "text"
      },
      "source": [
        "###**make_instance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7g2Fqt7_2z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_instance(args):\n",
        "    depot, loc, demand, capacity, *args = args\n",
        "    grid_size = 1\n",
        "    if len(args) > 0:\n",
        "        depot_types, customer_types, grid_size = args\n",
        "    return {\n",
        "        'loc': torch.tensor(loc, dtype=torch.float) / grid_size,\n",
        "        'demand': torch.tensor(demand, dtype=torch.float) / capacity,\n",
        "        'depot': torch.tensor(depot, dtype=torch.float) / grid_size\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99p5tayF_3KF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VRPDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, filename=None, size=50, num_samples=1000000, offset=0, distribution=None):\n",
        "        super(VRPDataset, self).__init__()\n",
        "\n",
        "        self.data_set = []\n",
        "        if filename is not None:\n",
        "            assert os.path.splitext(filename)[1] == '.pkl'\n",
        "\n",
        "            with open(filename, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            self.data = [make_instance(args) for args in data[offset:offset+num_samples]]\n",
        "\n",
        "        else:\n",
        "\n",
        "            # From VRP with RL paper https://arxiv.org/abs/1802.04240\n",
        "            CAPACITIES = {\n",
        "                10: 20.,\n",
        "                20: 30.,\n",
        "                50: 40.,\n",
        "                100: 50.\n",
        "            }\n",
        "\n",
        "            self.data = [\n",
        "                {\n",
        "                    'loc': torch.FloatTensor(size, 2).uniform_(0, 1),\n",
        "                    # Uniform 1 - 9, scaled by capacities\n",
        "                    'demand': (torch.FloatTensor(size).uniform_(0, 9).int() + 1).float() / CAPACITIES[size],\n",
        "                    'depot': torch.FloatTensor(2).uniform_(0, 1)\n",
        "                }\n",
        "                for i in range(num_samples)\n",
        "            ]\n",
        "\n",
        "        self.size = len(self.data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WggC5ZWlNbMp",
        "colab_type": "text"
      },
      "source": [
        "#**utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXQ0fNtB_3V0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "def check_extension(filename):\n",
        "    if os.path.splitext(filename)[1] != \".pkl\":\n",
        "        return filename + \".pkl\"\n",
        "    return filename\n",
        "\n",
        "\n",
        "def save_dataset(dataset, filename):\n",
        "\n",
        "    filedir = os.path.split(filename)[0]\n",
        "\n",
        "    if not os.path.isdir(filedir):\n",
        "        os.makedirs(filedir)\n",
        "\n",
        "    with open(check_extension(filename), 'wb') as f:\n",
        "        pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def load_dataset(filename):\n",
        "\n",
        "    with open(check_extension(filename), 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7147YRX_3cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oolbTtNd_3hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfUopGCP_3lF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RofcxowD_3pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKRsqi5__3t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iO_LCv9_3yN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_FuHjgX_32O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R7nj8n1_361",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrNfYnctNZvQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}